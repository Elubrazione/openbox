# 迁移学习


我们注意到在进行黑盒优化服务时，用户经常运行与以前类似的任务。
这个观察为我们提供了加速BBO的方法。
相比Vizier提供的针对单目标BBO问题的迁移学习功能相比，OpenBox实现了更为通用的迁移学习框架，具有以下优点：


1） 支持通用黑盒优化问题；
2） 与大多数贝叶斯优化方法兼容。

OpenBox 的输入是 $𝐾 + 1$ 个任务： $𝐾$ 个之前的任务 $D^1$, ...,
$D^𝐾$ 和一个当前的任务 $D^𝑇$。
每个任务 $D^𝑖 = \{(𝒙, 𝒚)\} ( 𝑖 = 1, ...,𝐾 )$ 包含一系列的观察。
注意到，$𝒚$ 是一个包含配置 $𝒙$ 对应多个目标结果的数组。
对于有 $𝑝$ 个目标的多目标优化问题，我们提出单独迁移和这 $𝑝$ 个目标相关的知识。
因此，多目标的迁移学习就被转化成了 $𝑝$ 个单目标的迁移学习。
我们采用以下的迁移学习技术：

1）我们首先对于每个先验任务 $𝐷^𝑖$ （这里 $𝑖$ 是第 $𝑖$ 个先验）都训练一个替代模型 $𝑀^𝑖$，也对于 $𝐷^𝑇$ 训练一个模型 $𝑀^𝑇$。

2）基于 $𝑀^{1:𝐾}$ 和 $𝑀^𝑇$ 我们通过结合所有的替代模型来构建一个迁移学习替代模型：$𝑀^{TL} = agg(\{𝑀^1, ...,𝑀^𝐾,𝑀^𝑇 \};w)$；

3）我们用替代模型 $𝑀^{TL}$ 来指导配置搜索，而不是用原始的模型 $𝑀^𝑇$。

具体来说，我们使用 gPoE 来组合多个基本的代理模型（agg），这里参数 $w$ 是根据构型排序计算出来的，它反映了源任务和目标任务之间的相似性。


## 性能对比
我们将OpenBox与主流的迁移学习基准方法Vizier和非迁移学习基准方法SMAC3进行对比。
每个算法的平均排名（Rank值越低越好）如下图所示。
有关实验设置、数据集信息和更多实验结果，请参阅我们的 [已发表文章]()。


<img src="../../imgs/tl_lightgbm_75_rank_result.svg" width="70%" class="align-center">

