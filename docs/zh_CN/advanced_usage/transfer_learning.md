# 迁移学习


我们注意到在进行黑盒优化服务时，用户经常运行与以前类似的任务。
这个观察为我们提供了加速BBO的方法。
相比Vizier提供的针对单目标BBO问题的迁移学习功能相比，OpenBox实现了更为通用的迁移学习框架，具有以下优点：


1) 支持通用黑盒优化问题；
2) 与大多数贝叶斯优化方法兼容。

OpenBox 的输入是 $K + 1$ 个任务： $K$ 个之前的任务 $D^1$, ...,
$D^K$ 和一个当前的任务 $D^T$。
每个任务 $D^i = \{(x, y)\} ( i = 1, ...,K )$ 包含一系列的观察。
注意到，$y$ 是一个包含配置 $x$ 对应多个目标结果的数组。
对于有 $p$ 个目标的多目标优化问题，我们提出单独迁移和这 $p$ 个目标相关的知识。
因此，多目标的迁移学习就被转化成了 $p$ 个单目标的迁移学习。
我们采用以下的迁移学习技术：

1) 我们首先对于每个先验任务 $D^i$ （这里 $i$ 是第 $i$ 个先验）都训练一个替代模型 $M^i$，也对于 $D^T$ 训练一个模型 $M^T$。

2) 基于 $M^{1:K}$ 和 $M^T$ 我们通过结合所有的替代模型来构建一个迁移学习替代模型：$M^{TL} = agg(\{M^1, ...,M^K,M^T \};w)$；

3) 我们用替代模型 $M^{TL}$ 来指导配置搜索，而不是用原始的模型 $M^T$。

具体来说，我们使用 gPoE 来组合多个基本的代理模型（agg），这里参数 $w$ 是根据构型排序计算出来的，它反映了源任务和目标任务之间的相似性。


## 性能对比
我们将OpenBox与主流的迁移学习基准方法Vizier和非迁移学习基准方法SMAC3进行对比。
每个算法的平均排名（Rank值越低越好）如下图所示。
有关实验设置、数据集信息和更多实验结果，请参阅我们的 [已发表文章](https://dl.acm.org/doi/abs/10.1145/3447548.3467061)。


<img src="../../imgs/tl_lightgbm_75_rank_result.svg" width="70%" class="align-center">

